# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
datahub-gms:
  enabled: true
  extraEnvs:
      - name: REACT_APP_LOGO_URL
        # value: https://nuodata.io/logo.png
        value: https://media.licdn.com/dms/image/C560BAQFuyE9nc31ZAg/company-logo_100_100/0/1674001211364?e=2147483647&v=beta&t=2igGDeHhGp53qZg3lf1y2af_NsFRMhEmnojuOHgj6hE
      - name: REACT_APP_FAVICON_URL
        # value: https://nuodata.io/logo.png
        value: https://media.licdn.com/dms/image/C560BAQFuyE9nc31ZAg/company-logo_100_100/0/1674001211364?e=2147483647&v=beta&t=2igGDeHhGp53qZg3lf1y2af_NsFRMhEmnojuOHgj6hE
  image:
    repository: linkedin/datahub-gms
    tag: "v0.10.0" #defaults to .global.datahub.version
  # resources:
  #   limits:
  #     memory: 1Gi
  #   requests:
  #     cpu: 10m
  #     memory: 512Mi
  service:
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: "external"
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "instance"
      service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"

datahub-frontend:
  enabled: true
  image:
    repository: linkedin/datahub-frontend-react
    # tag: "v0.10.0 # defaults to .global.datahub.version
  # resources:
  #   limits:
  #     memory: 1000Mi
  #   requests:
  #     cpu: 10m
  #     memory: 500Mi
  # Set up ingress to expose react front-end
  ingress:
    enabled: false
    apiVersion: "networking.k8s.io/v1"
    className: "nginx"
    annotations:
      # kubernetes.io/ingress.class: ingress-nginx
      # kubernetes.io/ingress.class: alb
    #   alb.ingress.kubernetes.io/scheme: internet-facing
    #   alb.ingress.kubernetes.io/target-type: instance
    #   #alb.ingress.kubernetes.io/certificate-arn: <<certificate-arn>>
    #   alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0
    #   alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
    #   #alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
    hosts:
      - host: localhost
        # redirectPaths:
        #  - path: /*
        #    name: ssl-redirect
        #    port: "80"
        paths:
          - /
              
  extraVolumes:
   - name: datahub-users
     secret:
       defaultMode: 0444
       secretName:  datahub-users-secret
  extraVolumeMounts:
   - name: datahub-users
     mountPath: datahub-frontend/conf/user.props
     subPath: user.props
  service:
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: "external"
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "instance"
      service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"

acryl-datahub-actions:
  enabled: true
  image:
    repository: acryldata/datahub-actions
    tag: "v0.0.11"
  # resources:
  #   limits:
  #     memory: 512Mi
  #   requests:
  #     cpu: 300m
  #     memory: 256Mi

datahub-mae-consumer:
  image:
    repository: linkedin/datahub-mae-consumer
    # tag: "v0.10.0 # defaults to .global.datahub.version
  # resources:
  #   limits:
  #     memory: 1536Mi
  #   requests:
  #     cpu: 100m
  #     memory: 256Mi

datahub-mce-consumer:
  image:
    repository: linkedin/datahub-mce-consumer
    # tag: "v0.10.0 # defaults to .global.datahub.version
  # resources:
  #   limits:
  #     memory: 1536Mi
  #   requests:
  #     cpu: 100m
  #     memory: 256Mi

elasticsearchSetupJob:
  enabled: true
  image:
    repository: linkedin/datahub-elasticsearch-setup
    # tag: "v0.10.0 # defaults to .global.datahub.version
  #extraEnvs:
  #  - name: USE_AWS_ELASTICSEARCH
  #    value: "true"
  # resources:
  #   limits:
  #     cpu: 500m
  #     memory: 512Mi
  #   requests:
  #     cpu: 300m
  #     memory: 256Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

kafkaSetupJob:
  enabled: true
  image:
    repository: linkedin/datahub-kafka-setup
    # tag: "v0.10.0 # defaults to .global.datahub.version
  # resources:
  #   limits:
  #     cpu: 500m
  #     memory: 1024Mi
  #   requests:
  #     cpu: 300m
  #     memory: 768Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

mysqlSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-mysql-setup
    # tag: "v0.10.0 # defaults to .global.datahub.version
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

postgresqlSetupJob:
  enabled: false
  image:
    repository: acryldata/datahub-postgres-setup
    # tag: "v0.10.0 # defaults to .global.datahub.version
  # resources:
  #   limits:
  #     cpu: 500m
  #     memory: 512Mi
  #   requests:
  #     cpu: 300m
  #     memory: 256Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

datahubUpgrade:
  enabled: true
  image:
    repository: acryldata/datahub-upgrade
    # tag: "v0.10.0 # defaults to .global.datahub.version
  batchSize: 1000
  batchDelayMs: 100
  noCodeDataMigration:
    sqlDbType: "MYSQL"
  podSecurityContext: {}
  securityContext: {}
  podAnnotations: {}
  restoreIndices:
    # resources:
    #   limits:
    #     cpu: 500m
    #     memory: 512Mi
    #   requests:
    #     cpu: 300m
    #     memory: 256Mi

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
## ONLY APPLICABLE FOR VERSIONS 0.10.0 AND ABOVE
datahubSystemUpdate:
  image:
    repository: acryldata/datahub-upgrade
  podSecurityContext: {}
  securityContext: {}
  podAnnotations: {}
  # resources:
  #   limits:
  #     cpu: 500m
  #     memory: 512Mi
  #   requests:
  #     cpu: 300m
  #     memory: 256Mi

global:
  strict_mode: true
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false

  elasticsearch:
    host: "elasticsearch-master"
    port: "9200"
    insecure: "false"
    useSSL: "false"
    #auth:
    #  username: <<username>>
    #  password:
    #    secretRef: elasticsearch-secrets
    #    secretKey: elasticsearch-password

    ## The following section controls when and how reindexing of elasticsearch indices are performed
    ## ONLY APPLICABLE FOR VERSIONS 0.10.0 AND ABOVE
    index:
      ## Enable reindexing when mappings change based on the data model annotations
      enableMappingsReindex: true

      ## Enable reindexing when static index settings change.
      ## Dynamic settings which do not require reindexing are not affected
      ## Primarily this should be enabled when re-sharding is necessary for scaling/performance.
      enableSettingsReindex: true

      ## Index settings can be overridden for entity indices or other indices on an index by index basis
      ## Some index settings, such as # of shards, requires reindexing while others, i.e. replicas, do not
      ## Non-Entity indices do not require the prefix
      # settingsOverrides: '{"graph_service_v1":{"number_of_shards":"5"},"system_metadata_service_v1":{"number_of_shards":"5"}}'
      ## Entity indices do not require the prefix or suffix
      # entitySettingsOverrides: '{"dataset":{"number_of_shards":"10"}}'

      ## The amount of delay between indexing a document and having it returned in queries
      ## Increasing this value can improve performance when ingesting large amounts of data
      # refreshIntervalSeconds: 1

      ## The following options control settings for datahub-upgrade job when creating or reindexing indices
      upgrade:
        ## When reindexing is required, this option will clone the existing index as a backup
        ## The clone indices are not currently managed.
        cloneIndices: true

        ## Typically when reindexing the document counts between the original and destination indices should match.
        ## In some cases reindexing might not be able to proceed due to incompatibilities between a document in the
        ## orignal index and the new index's mappings. This document could be dropped and re-ingested or restored from
        ## the SQL database.
        ##
        ## This setting allows continuing if and only if the cloneIndices setting is also enabled which
        ## ensures a complete backup of the original index is preserved.
        allowDocCountMismatch: false

  kafka:
    bootstrap:
        server: "prerequisites-kafka:9092"
    zookeeper:
        server: "prerequisites-zookeeper:2181"
    topics:
      metadata_change_event_name: "MetadataChangeEvent_v4"
      failed_metadata_change_event_name: "FailedMetadataChangeEvent_v4"
      metadata_audit_event_name: "MetadataAuditEvent_v4"
      datahub_usage_event_name: "DataHubUsageEvent_v1"
      metadata_change_proposal_topic_name: "MetadataChangeProposal_v1"
      failed_metadata_change_proposal_topic_name: "FailedMetadataChangeProposal_v1"
      metadata_change_log_versioned_topic_name: "MetadataChangeLog_Versioned_v1"
      metadata_change_log_timeseries_topic_name: "MetadataChangeLog_Timeseries_v1"
      platform_event_topic_name: "PlatformEvent_v1"
      datahub_upgrade_history_topic_name: "DataHubUpgradeHistory_v1"
    ## For AWS MSK set this to a number larger than 1. Preferrably 3.
    partitions: 1
    replicationFactor: 1
    schemaregistry:
      url: "http://prerequisites-cp-schema-registry:8081"
      type: KAFKA

  sql:
    datasource:
      host: "prerequisites-mysql:3306"
      hostForMysqlClient: "prerequisites-mysql"
      port: "3306"
      url: "jdbc:mysql://prerequisites-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      driver: "com.mysql.jdbc.Driver"
      username: "root"
      password:
        secretRef: mysql-secrets
        secretKey: mysql-root-password

  datahub:
    version: v0.10.0
    gms:
      port: "8080"
      nodePort: "30001"

    monitoring:
      enablePrometheus: true

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0"
    ## SET IT TO TRUE FOR VERSIONS 0.10.0 AND ABOVE
    systemUpdate:
      ## The following options control settings for datahub-upgrade job which will
      ## managed ES indices and other update related work
      enabled: true

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret:
        enabled: true
        autoGenerate: true
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    encryptionKey: <encryption key value>

    managed_ingestion:
      enabled: true
      defaultCliVersion: "0.10.0"

    metadata_service_authentication:
      enabled: true
      #systemClientId: "__datahub_system"
      #systemClientSecret:
      #  secretRef: "datahub-auth-secrets"
      #  secretKey: "token_service_signing_key"
      #tokenService:
      #  signingKey:
      #    secretRef: "datahub-auth-secrets"
      #    secretKey: "token_service_signing_key"
      #  salt:
      #    secretRef: "datahub-auth-secrets"
      #    secretKey: "token_service_salt"
      # Set to false if you'd like to provide your own auth secrets
      provisionSecrets:
       enabled: false
      #  autoGenerate: true
      # Only specify if autoGenerate set to false
        # secretValues:
        #  secret: "secret_key"
        #  signingKey: "secret_key"
        #  salt: "secret_key"
